{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer Learning",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "copyright",
        "tmD8WSZDhAeR"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PLP9Q30PKtv"
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5W9rkuBmBu9"
      },
      "source": [
        "# Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXaViQKr8_LK"
      },
      "source": [
        "In the field of deep learning, **transfer learning** is defined as the conveyance of knowledge from one pretrained model to a new model. This simply means that transfer learning uses a pretrained model to train a new model. Typically the new model will have a more specific application than the pre-trained model.\n",
        "\n",
        "*Note that this lab is largely based on [an excellent transfer learning lab](https://www.tensorflow.org/tutorials/images/transfer_learning) from TensorFlow.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UWnlfCWDG1a"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXfX-YoQDLYM"
      },
      "source": [
        "The [`tensorflow_datasets`](https://www.tensorflow.org/datasets/api_docs/python/tfds) package has a [catalog of datasets](https://www.tensorflow.org/datasets/catalog/overview) that are easy to load into your TensorFlow environment for experimentation.\n",
        "\n",
        "In this lab we'll work with the [`cats_vs_dogs`](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs) dataset. This dataset contains thousands of images of cats and dogs. Looking at the [documentation for the dataset](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs), we can see there are `23,262` examples in the 'train' split of data. There are no test and validation splits.\n",
        "\n",
        "We could just load this one split directly and then split the data once we download it. Another option is to tell `tfds.load()` to split the data for us. To do that we must specify the splits.\n",
        "\n",
        "There is a specific notation we can use that tells the function how much of the data we want in each split. For instance `'train[:80%]'` indicates that we want the first `80%` of the train split in one tranche. `'train[80%:90%]'` indicates that we want the next 10% of the data in another tranche, and so on. You can see this at work in our `split` example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIQRzs7_1U-R"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
        "    'cats_vs_dogs',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPSYyMbMOptP"
      },
      "source": [
        "The metadata returned from our dataset contains useful information about the data. For instance, it includes the number of classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_BUimcNOufF"
      },
      "source": [
        "metadata.features['label'].num_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItdkkKmxPB56"
      },
      "source": [
        "And the class names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci7G9seIPDSR"
      },
      "source": [
        "metadata.features['label'].names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1OMxk5WPJr7"
      },
      "source": [
        "It even comes with some handy functions for converting between class names and numbers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6mOsZr1PRMA"
      },
      "source": [
        "print(metadata.features['label'].int2str(1))\n",
        "print(metadata.features['label'].str2int('cat'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuYfsy0APlxa"
      },
      "source": [
        "Let's store the `int2str` into a more conveniently named function for later use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us7d6Xy6PsVp"
      },
      "source": [
        "get_class_name = metadata.features['label'].int2str\n",
        "get_class_name(0), get_class_name(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAiYymNUIFNn"
      },
      "source": [
        "Let's take a quick look at our dataset. First we'll peek at the shape of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw5r5BI61kky"
      },
      "source": [
        "raw_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCeBgxY-ISAu"
      },
      "source": [
        "`(None, None, 3)` lets us know that we have three channel images, but we aren't sure of the lengths and widths. They are likely different depending on the image. We also don't know how many images we have.\n",
        "\n",
        "Let's do some deeper analysis.\n",
        "\n",
        "It turns out that you can iterate over a `DatasetV1Adapter` with a standard `for` loop. The items returned at each iteration are the image and the label.\n",
        "\n",
        "We'll create a helper function to analyze a split of our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjXEqFe2FRyj"
      },
      "source": [
        "import collections\n",
        "\n",
        "\n",
        "def split_details(split):\n",
        "  counts = collections.defaultdict(int)\n",
        "  for image, label in split:\n",
        "    counts[label.numpy()]+=1\n",
        "\n",
        "  total = 0\n",
        "  for cls, cnt in counts.items():\n",
        "    print(f\"Class {get_class_name(cls)}: {cnt}\")\n",
        "    total += cnt\n",
        "  \n",
        "  print(f\"Total: {total}\")\n",
        "\n",
        "for s in (\n",
        "    (\"Train\", raw_train),\n",
        "    (\"Validation\", raw_validation),\n",
        "    (\"Test\", raw_test)):\n",
        "  print(s[0])\n",
        "  split_details(s[1])\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PXX0P1pKZcq"
      },
      "source": [
        "We'll train on `18,610` examples, validating on `2,326`, and performing our final testing on `2,326`. Our classes look pretty evenly spread across all of the splits. The classes also seem to have a similar number of total examples.\n",
        "\n",
        "Let's now see what our images look like. We'll display one dog and one cat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZptUQCfFFPc6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "for cls in (0, 1):\n",
        "  for image, label in raw_train:\n",
        "    if label == cls:\n",
        "      plt.figure()\n",
        "      plt.imshow(image)\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S9_nR8tL7eh"
      },
      "source": [
        "These are color images with noisy backgrounds. Also, the images aren't the same size, so we'll need to eventually resize them to feed our model.\n",
        "\n",
        "Let's find the range of color values and image sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ8jaTmwQqxD"
      },
      "source": [
        "import sys \n",
        "\n",
        "global_min = sys.maxsize \n",
        "global_max = -sys.maxsize-1\n",
        "sizes = collections.defaultdict(int)\n",
        "\n",
        "for split in (raw_train, raw_validation, raw_test):\n",
        "  for image, _ in split:\n",
        "    local_max = image.numpy().max()\n",
        "    local_min = image.numpy().min()\n",
        "    sizes[image.numpy().shape] += 1\n",
        "\n",
        "    if local_max > global_max:\n",
        "      global_max = local_max\n",
        "    \n",
        "    if local_min < global_min:\n",
        "      global_min = local_min\n",
        "\n",
        "print(f\"Color values range from {global_min} to {global_max}\")\n",
        "resolutions = [x[0] for x in sorted(sizes.items(), key=lambda r: r[0])]\n",
        "print(f\"There are {len(resolutions)} resolutions ranging from \",\n",
        "      f\"{resolutions[0]} to {resolutions[-1]}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8120tpjjR9kg"
      },
      "source": [
        "It looks like we are dealing with color values from `0` through `255`, which is pretty standard.\n",
        "\n",
        "We have a huge number of different resolutions. There are over `6,000` different image sizes in this dataset, some as small as `4x4x3`! It is difficult to imagine that an image that small would be meaningful. Let's see how many tiny images we are dealing with.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZNs0y4QVbfV"
      },
      "source": [
        "for resolution in sorted(sizes.items(), key=lambda r: r[0])[:10]:\n",
        "  print(resolution[0], ': ', resolution[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PB-kkRdV-qp"
      },
      "source": [
        "There is only one truly tiny image. Let's take a look at it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyE0awq0WK7f"
      },
      "source": [
        "shown = False\n",
        "for split in (raw_train, raw_validation, raw_test):\n",
        "  if shown:\n",
        "    break\n",
        "  for image, _ in split:\n",
        "    if image.numpy().shape == (4, 4, 3):\n",
        "      plt.figure()\n",
        "      plt.imshow(image)\n",
        "      shown = True\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RnfdYnGWpf6"
      },
      "source": [
        "That's definitely bad data. Let's go ahead and sample some of the other small images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIiBJoILWgZp"
      },
      "source": [
        "for split in (raw_train, raw_validation, raw_test):\n",
        "  for image, _ in split:\n",
        "    if image.numpy().shape[0] < 50 and image.numpy().shape[0] > 4:\n",
        "      plt.figure()\n",
        "      plt.imshow(image)      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5FKNGraW9a-"
      },
      "source": [
        "Though some are difficult to interpret, you can probably tell that each image contains either cats or dogs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT1sgXpiopxy"
      },
      "source": [
        "In order to not process the tiny image, we can write a filter function. We know the shape is `(4, 4, 3)`, so we can filter for that exact shape. To make the filter a little more generic, we'll instead filter out any image that is shorter or narrower than `6` pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQDouruunYk7"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def filter_out_small(image, _):\n",
        "  return tf.math.reduce_any(tf.shape(image)[0] > 5 and tf.shape(image)[1] > 5)\n",
        "\n",
        "for s in (\n",
        "    (\"Train\", raw_train.filter(filter_out_small)),\n",
        "    (\"Validation\", raw_validation.filter(filter_out_small)),\n",
        "    (\"Test\", raw_test.filter(filter_out_small))):\n",
        "  print(s[0])\n",
        "  split_details(s[1])\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAOR8oQ7yIMO"
      },
      "source": [
        "It looks like our problematic image was a cat in the holdout test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vLVLZdySHj4"
      },
      "source": [
        "## The Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYBHZvDZSNLx"
      },
      "source": [
        "To build our cat/dog classifier, we'll use the learnings of a pre-trained model. Specifically [`MobileNetV2`](https://arxiv.org/abs/1801.04381). We'll use [`tf.keras.applications.MobileNetV2`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2) to load the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF0-RdJUqptF"
      },
      "source": [
        "### Model-Specific Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYJM80kQqnX7"
      },
      "source": [
        "[Researching `MobileNetV2`](https://arxiv.org/pdf/1801.04381.pdf), you'll find that the neural network by default takes an input of image of size `(224, 224, 3)`. Though the model can be configured to take other inputs, all of our images are different sizes. So we might as well resize them to fit the default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EraGNJ-a4ER_"
      },
      "source": [
        "IMG_SIZE = 224 \n",
        "\n",
        "def resize_images(image, label):\n",
        "  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "  return image, label\n",
        "\n",
        "train_resized = raw_train.map(resize_images)\n",
        "validation_resized = raw_validation.map(resize_images)\n",
        "test_resized = raw_test.map(resize_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izrQTKugq-F6"
      },
      "source": [
        "We also need to normalize our data, but what should our input values be scaled to? Ideally our input data should look like the input data that the `MobileNetV2` was trained on. Unfortunately, this isn't published.\n",
        "\n",
        "`MobileNetV2` [internally](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v2.py) uses [`relu6`](https://www.tensorflow.org/api_docs/python/tf/nn/relu6), which limits activation outputs to the range of `0` through `6`. This hints that we might want to normalize our values between `[0, 1]` or even `[0, 6]`.\n",
        "\n",
        "It also performs **batch normalization** throughout the network. This is the process of dividing input values by the mean and subtracting the standard deviation of each batch of data processed. So \"batch normalization\" is really \"batch standardization\".\n",
        "\n",
        "Standardizing our data by batch is possible. We could also calculate the mean and standard deviation of all of the data and standardize the entire dataset in one pass. Or we could *approximate* standardization and simply divide our input data by `127.5` (the midpoint of our `[0, 255]` range) and then subtract `1` (a guessed standard deviation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fslYHkuJ4IPx"
      },
      "source": [
        "def standardize_images(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = (image/127.5) - 1\n",
        "  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "  return image, label\n",
        "\n",
        "train_standardized = train_resized.map(standardize_images)\n",
        "validation_standardized = validation_resized.map(standardize_images)\n",
        "test_standardized = test_resized.map(standardize_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZcdAv-Pse18"
      },
      "source": [
        "Did it work? Let's check it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LOehxAGsniN"
      },
      "source": [
        "import sys \n",
        "\n",
        "global_min = sys.maxsize \n",
        "global_max = -sys.maxsize-1\n",
        "sizes = collections.defaultdict(int)\n",
        "\n",
        "for split in (train_standardized, validation_standardized, test_standardized):\n",
        "  for image, _ in split:\n",
        "    local_max = image.numpy().max()\n",
        "    local_min = image.numpy().min()\n",
        "    sizes[image.numpy().shape] += 1\n",
        "\n",
        "    if local_max > global_max:\n",
        "      global_max = local_max\n",
        "    \n",
        "    if local_min < global_min:\n",
        "      global_min = local_min\n",
        "\n",
        "print(f\"Color values range from {global_min} to {global_max}\")\n",
        "resolutions = [x[0] for x in sorted(sizes.items(), key=lambda r: r[0])]\n",
        "print(f\"There are {len(resolutions)} resolutions ranging from \",\n",
        "      f\"{resolutions[0]} to {resolutions[-1]}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqwTjL4Js-SW"
      },
      "source": [
        "Looks great! Now it is time to load our pretrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7DADZBitDv0"
      },
      "source": [
        "### Loading MobileNetV2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh4pFkiatZG_"
      },
      "source": [
        "Loading `MobileNetV2` is pretty straight-forward.\n",
        "\n",
        "We need to pass in the input shape, which is `(224, 224, 3)` for each image.\n",
        "\n",
        "We also include pre-trained weights based on [`ImageNet`](http://www.image-net.org/). This is where the transfer learning comes in. `ImageNet` has over a million images that map to a thousand labels. `MobileNetV2` has been trained on `ImageNet`. We'll use those learnings and then add a few more layers of our own model to build a cat/dog classifier.\n",
        "\n",
        "The final argument is `include_top`. Typically when building a classification model, toward the end of the model, high-dimensional layers are flattened down into two-dimensional tensors. This is considered the top of the model since diagrams often show the final layers at the top. For transfer learning we'll leave this dimensionality reduction off.\n",
        "\n",
        "If you do include the top of the model, the following extra layers will be shown:\n",
        "\n",
        "```text    \n",
        "__________________________________________________________________________________________________\n",
        "global_average_pooling2d_1 (Glo (None, 1280)         0           out_relu[0][0]                   \n",
        "__________________________________________________________________________________________________\n",
        "predictions (Dense)             (None, 1000)         1281000     global_average_pooling2d_1[0][0] \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jezbRG_lsjNW"
      },
      "source": [
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "mnv2 = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                         weights='imagenet',\n",
        "                                         include_top=False)\n",
        "\n",
        "mnv2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogAS920-wMPn"
      },
      "source": [
        "It is often a good idea to \"freeze\" the trained model. This prevents its weights from being updated when we train our new model.\n",
        "\n",
        "It is really only recommended to update the weights of the pretrained model when you are about to train on a large and similar dataset, as compared to the one that was originally trained on. This is not the case in our example. `ImageNet` has a thousand classes and over a million images. We have two classes and a few thousand images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4pxUcpFwD72"
      },
      "source": [
        "mnv2.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUmCzu-MxJpp"
      },
      "source": [
        "### Batching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PpC7peUxPVi"
      },
      "source": [
        "We will want to train our model in batches. In our case we'll use a batch size of `32`. You might want to experiment with other sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRLy4sdM4PcB"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "\n",
        "train_batches = train_standardized.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "validation_batches = validation_standardized.batch(BATCH_SIZE)\n",
        "test_batches = test_standardized.filter(filter_out_small).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBDaCXb7xjRQ"
      },
      "source": [
        "You can see that we now have a well-defined input shape for each training batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3Vjs4S-4ZRR"
      },
      "source": [
        "image_batch, label_batch = next(iter(train_batches.take(1)))\n",
        "\n",
        "image_batch.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCByIZ6BykVB"
      },
      "source": [
        "If we apply our model to our first batch, you can see that we get a `(32, 7, 7, 1280)` block of features. These will be the input to our cat/dog model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en5H-9fh5eqN"
      },
      "source": [
        "feature_batch = mnv2(image_batch)\n",
        "print(feature_batch.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su6ldmrnzR0W"
      },
      "source": [
        "### Extending the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ_mNo045qlM"
      },
      "source": [
        "Now we can perform the actual transfer learning. We'll build a new model that classifies images as containing dogs or cats. In order to do that, we can use a `Sequential` model with the pretrained model as the first layer.\n",
        "\n",
        "Note that the output layer of our pretrained model is:\n",
        "\n",
        "```text\n",
        "out_relu (ReLU)                 (None, 7, 7, 1280)   0           Conv_1_bn[0][0]  \n",
        "```\n",
        "\n",
        "Since the activation function is `relu6`, we know that the data that we'll be receiving is in the range of `[0, 6]`. We apply a pooling layer to reduce our inputs. In our output layer, we distill the inputs down to a single number that indicates if the image is of a cat or dog. We chose the sigmoid function, which will cause the output to be in a range of `[0, 1]`. This represents the confidence in an image being a dog, since dog is encoded as `1`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJOpWMn5zay6"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  mnv2,\n",
        "  tf.keras.layers.GlobalAveragePooling2D(),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaSD-7hr9qx4"
      },
      "source": [
        "We now compile the model, training for accuracy with binary cross entropy used to calculate loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNn8X9r8zjE1"
      },
      "source": [
        "model.compile(\n",
        "  loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odoqqyzL9yjH"
      },
      "source": [
        "Training will take a few minutes. Be sure to use GPU or it will take a really long time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz2Z1y6L6cyI"
      },
      "source": [
        "history = model.fit(\n",
        "  train_batches,\n",
        "  epochs=10,\n",
        "  validation_data=validation_batches\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiL5fu2W-nG5"
      },
      "source": [
        "We got a training accuracy of over `99%` and a validation accuracy close to `99%`! Let's graph the accuracy and loss per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znoDaMev830w"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEJxZoCm-0xf"
      },
      "source": [
        "The graph makes it look like we might be overfitting, but if you look at the range on the y-axis, we actually aren't doing too badly. We should, however, perform a final test to see if we can generalize well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQsFrH8k_OVD"
      },
      "source": [
        "model.evaluate(test_batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbKT0oqU_wRe"
      },
      "source": [
        "We got an accuracy of just over `99%`, which can give us some confidence that this model generalizes well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7VptkBN__s8"
      },
      "source": [
        "### Making Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoRc2-8bACqo"
      },
      "source": [
        "We can use the model to make predictions by using the `predict()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8atvraZy_2kU"
      },
      "source": [
        "predictions = model.predict(test_batches)\n",
        "\n",
        "predictions.min(), predictions.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtSukjLWA5i3"
      },
      "source": [
        "Remember the predictions can range from `0.0` to `1.0`. We can round them and cast them to integers to get class mappings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kirLe5Gy_54B"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "predictions = np.round(predictions.flatten(), 0).astype(np.int)\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBtScX8zC-8e"
      },
      "source": [
        "And we can now print the predicted class alongside the original image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuadhqEaBC5-"
      },
      "source": [
        "print(get_class_name(predictions[0]))\n",
        "_ = plt.imshow(next(iter(raw_test.take(1)))[0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0xj8vECoQq1"
      },
      "source": [
        "You can also make predictions by calling the model directly and passing it a single batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsUt3fBcoTOJ"
      },
      "source": [
        "predictions = model(next(iter(test_batches)))\n",
        "predictions = np.round(predictions, 0).astype(np.int).flatten()\n",
        "\n",
        "print(get_class_name(predictions[0]))\n",
        "_ = plt.imshow(next(iter(raw_test.take(1)))[0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yww1QiWWDN0r"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-BKV0dDDPXk"
      },
      "source": [
        "## Exercise 1: Food 101"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq2E02AoevMb"
      },
      "source": [
        "In this exercise you'll build a classifier for the [Food 101](https://www.tensorflow.org/datasets/catalog/food101) dataset. The classifier will transfer learnings from [`DenseNet201`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/DenseNet201).\n",
        "\n",
        "In order to complete the exercise, you will need to:\n",
        "* Load the [Food 101](https://www.tensorflow.org/datasets/catalog/food101) dataset. *Be sure to pay attention to the splits!*\n",
        "* Perform exploratory data analysis on the dataset.\n",
        "* Ensure every class is represented in your train, test, and validation splits of the dataset.\n",
        "* Normalize or standardize your data in the way that the model was trained. *You can find this information in the [paper introducing the model](https://arxiv.org/pdf/1608.06993.pdf).*\n",
        "* Extend [`DenseNet201`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/DenseNet201) with a new model, and have it classify the `101` food types. *Note that [`one_hot`](https://www.tensorflow.org/api_docs/python/tf/one_hot) and [`Dataset.map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) can help you manipulate the targets to make the model train faster.*\n",
        "* Graph training accuracy and loss.\n",
        "* Calculate accuracy and loss for your holdout test set.*\n",
        "* Make predictions and print out one predicted label and original image.\n",
        "\n",
        "> **Don't sweat too much about your model's performance. We were only able to get about `75%` training accuracy (with obvious overfitting) in our naive model after `10` training epochs. This model is trying to classify `101` different things with messy images. Don't expect it to perform anywhere close to our binary model above.*\n",
        "\n",
        "Use as many code and text cells as you need to complete this task. Explain your work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgqpSFHwg3J6"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fscnp0FJg-b5"
      },
      "source": [
        "# Your Solution Goes Here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeI-54sndV5R"
      },
      "source": [
        "#### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5mfZ8m3dV5V"
      },
      "source": [
        "The [`tensorflow_datasets`](https://www.tensorflow.org/datasets/api_docs/python/tfds) package has a [catalog of datasets](https://www.tensorflow.org/datasets/catalog/overview) that are easy to load into your TensorFlow environment for experimentation.\n",
        "\n",
        "In this lab we'll work with the [`cats_vs_dogs`](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs) dataset. This dataset contains thousands of images of cats and dogs. Looking at the [documentation for the dataset](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs), we can see there are `23,262` examples in the 'train' split of data. There are no test and validation splits.\n",
        "\n",
        "We could just load this one split directly and then split the data once we download it. Another option is to tell `tfds.load()` to split the data for us. To do that we must specify the splits.\n",
        "\n",
        "There is a specific notation we can use that tells the function how much of the data we want in each split. For instance `'train[:80%]'` indicates that we want the first `80%` of the train split in one tranche. `'train[80%:90%]'` indicates that we want the next 10% of the data in another tranche, and so on. You can see this at work in our `split` example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFP8uj7UdV5V"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
        "    'food101',\n",
        "    split=['train[:80%]', 'train[80%:]', 'validation'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxmez-4-dV5V"
      },
      "source": [
        "The metadata returned from our dataset contains useful information about the data. For instance, it includes the number of classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8bDD6D6dV5W"
      },
      "source": [
        "metadata.features['label'].num_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGQMTV5HdV5W"
      },
      "source": [
        "And the class names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLEIRxQpdV5W"
      },
      "source": [
        "metadata.features['label'].names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f86Qfm4dV5W"
      },
      "source": [
        "It even comes with some handy functions for converting between class names and numbers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eCPve6KdV5W"
      },
      "source": [
        "print(metadata.features['label'].int2str(1))\n",
        "print(metadata.features['label'].str2int('apple_pie'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo9WIJHmdV5X"
      },
      "source": [
        "Let's store the `int2str` into a more conveniently named function for later use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHTsxuCydV5Y"
      },
      "source": [
        "get_class_name = metadata.features['label'].int2str\n",
        "get_class_name(0), get_class_name(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kYbOGaYdV5Y"
      },
      "source": [
        "raw_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUV2jnledV5Z"
      },
      "source": [
        "import collections\n",
        "\n",
        "\n",
        "def split_details(split):\n",
        "  counts = collections.defaultdict(int)\n",
        "  for image, label in split:\n",
        "    counts[label.numpy()]+=1\n",
        "\n",
        "  total = 0\n",
        "  for cls, cnt in counts.items():\n",
        "    print(f\"Class {get_class_name(cls)}: {cnt}\")\n",
        "    total += cnt\n",
        "  \n",
        "  print(f\"Total: {total}\")\n",
        "\n",
        "for s in (\n",
        "    (\"Train\", raw_train),\n",
        "    (\"Validation\", raw_validation),\n",
        "    (\"Test\", raw_test)):\n",
        "  print(s[0])\n",
        "  split_details(s[1])\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa6k7t_IdV5Z"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "for cls in (0, 1):\n",
        "  for image, label in raw_train:\n",
        "    if label == cls:\n",
        "      plt.figure()\n",
        "      plt.imshow(image)\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHalNqQodV5Z"
      },
      "source": [
        "import sys \n",
        "\n",
        "global_min = sys.maxsize \n",
        "global_max = -sys.maxsize-1\n",
        "sizes = collections.defaultdict(int)\n",
        "\n",
        "for split in (raw_train, raw_validation, raw_test):\n",
        "  for image, _ in split:\n",
        "    local_max = image.numpy().max()\n",
        "    local_min = image.numpy().min()\n",
        "    sizes[image.numpy().shape] += 1\n",
        "\n",
        "    if local_max > global_max:\n",
        "      global_max = local_max\n",
        "    \n",
        "    if local_min < global_min:\n",
        "      global_min = local_min\n",
        "\n",
        "print(f\"Color values range from {global_min} to {global_max}\")\n",
        "resolutions = [x[0] for x in sorted(sizes.items(), key=lambda r: r[0])]\n",
        "print(f\"There are {len(resolutions)} resolutions ranging from \",\n",
        "      f\"{resolutions[0]} to {resolutions[-1]}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN6-ms8udV5a"
      },
      "source": [
        "for resolution in sorted(sizes.items(), key=lambda r: r[0])[:10]:\n",
        "  print(resolution[0], ': ', resolution[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3mUhz3sdV5a"
      },
      "source": [
        "shown = False\n",
        "for split in (raw_train, raw_validation, raw_test):\n",
        "  if shown:\n",
        "    break\n",
        "  for image, _ in split:\n",
        "    if image.numpy().shape == (122, 512, 3):\n",
        "      plt.figure()\n",
        "      plt.imshow(image)\n",
        "      shown = True\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYf6EjowdV5b"
      },
      "source": [
        "#### The Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud_9mdXzdV5b"
      },
      "source": [
        "To build our cat/dog classifier, we'll use the learnings of a pre-trained model. Specifically [`MobileNetV2`](https://arxiv.org/abs/1801.04381). We'll use [`tf.keras.applications.MobileNetV2`](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2) to load the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRx2JYtldV5b"
      },
      "source": [
        "##### Model-Specific Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1yA3PN9dV5b"
      },
      "source": [
        "[Researching `MobileNetV2`](https://arxiv.org/pdf/1801.04381.pdf), you'll find that the neural network by default takes an input of image of size `(224, 224, 3)`. Though the model can be configured to take other inputs, all of our images are different sizes. So we might as well resize them to fit the default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RJPbqXddV5b"
      },
      "source": [
        "IMG_SIZE = 224 \n",
        "\n",
        "def resize_images(image, label):\n",
        "  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "  return image, label\n",
        "\n",
        "train_resized = raw_train.map(resize_images)\n",
        "validation_resized = raw_validation.map(resize_images)\n",
        "test_resized = raw_test.map(resize_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8QWpXyJdV5b"
      },
      "source": [
        "We also need to normalize our data, but what should our input values be scaled to? Ideally our input data should look like the input data that the `MobileNetV2` was trained on. Unfortunately, this isn't published.\n",
        "\n",
        "`MobileNetV2` [internally](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v2.py) uses [`relu6`](https://www.tensorflow.org/api_docs/python/tf/nn/relu6), which limits activation outputs to the range of `0` through `6`. This hints that we might want to normalize our values between `[0, 1]` or even `[0, 6]`.\n",
        "\n",
        "It also performs **batch normalization** throughout the network. This is the process of dividing input values by the mean and subtracting the standard deviation of each batch of data processed. So \"batch normalization\" is really \"batch standardization\".\n",
        "\n",
        "Standardizing our data by batch is possible. We could also calculate the mean and standard deviation of all of the data and standardize the entire dataset in one pass. Or we could *approximate* standardization and simply divide our input data by `127.5` (the midpoint of our `[0, 255]` range) and then subtract `1` (a guessed standard deviation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNTLVBrHdV5b"
      },
      "source": [
        "def standardize_images(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = (image/127.5) - 1\n",
        "  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "  return image, label\n",
        "\n",
        "train_standardized = train_resized.map(standardize_images)\n",
        "validation_standardized = validation_resized.map(standardize_images)\n",
        "test_standardized = test_resized.map(standardize_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn_djNzcdV5b"
      },
      "source": [
        "Did it work? Let's check it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S4Iz67PdV5b"
      },
      "source": [
        "import sys \n",
        "\n",
        "global_min = sys.maxsize \n",
        "global_max = -sys.maxsize-1\n",
        "sizes = collections.defaultdict(int)\n",
        "\n",
        "for split in (train_standardized, validation_standardized, test_standardized):\n",
        "  for image, _ in split:\n",
        "    local_max = image.numpy().max()\n",
        "    local_min = image.numpy().min()\n",
        "    sizes[image.numpy().shape] += 1\n",
        "\n",
        "    if local_max > global_max:\n",
        "      global_max = local_max\n",
        "    \n",
        "    if local_min < global_min:\n",
        "      global_min = local_min\n",
        "\n",
        "print(f\"Color values range from {global_min} to {global_max}\")\n",
        "resolutions = [x[0] for x in sorted(sizes.items(), key=lambda r: r[0])]\n",
        "print(f\"There are {len(resolutions)} resolutions ranging from \",\n",
        "      f\"{resolutions[0]} to {resolutions[-1]}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLEW9bMbdV5b"
      },
      "source": [
        "Looks great! Now it is time to load our pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lMU6H9zYPCK"
      },
      "source": [
        "# def onehot(image, label,): # change label to classes\n",
        "vectorized_classes = tf.one_hot(metadata.features['label'].names, metadata.features['label'].num_classes)\n",
        "  # return images, vectorized_classes\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7jfiyaCdV5c"
      },
      "source": [
        "##### Loading DenseNetV2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_sHVVSVdV5c"
      },
      "source": [
        "Loading `MobileNetV2` is pretty straight-forward.\n",
        "\n",
        "We need to pass in the input shape, which is `(224, 224, 3)` for each image.\n",
        "\n",
        "We also include pre-trained weights based on [`ImageNet`](http://www.image-net.org/). This is where the transfer learning comes in. `ImageNet` has over a million images that map to a thousand labels. `MobileNetV2` has been trained on `ImageNet`. We'll use those learnings and then add a few more layers of our own model to build a cat/dog classifier.\n",
        "\n",
        "The final argument is `include_top`. Typically when building a classification model, toward the end of the model, high-dimensional layers are flattened down into two-dimensional tensors. This is considered the top of the model since diagrams often show the final layers at the top. For transfer learning we'll leave this dimensionality reduction off.\n",
        "\n",
        "If you do include the top of the model, the following extra layers will be shown:\n",
        "\n",
        "```text    \n",
        "__________________________________________________________________________________________________\n",
        "global_average_pooling2d_1 (Glo (None, 1280)         0           out_relu[0][0]                   \n",
        "__________________________________________________________________________________________________\n",
        "predictions (Dense)             (None, 1000)         1281000     global_average_pooling2d_1[0][0] \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv4o1ILFdV5c"
      },
      "source": [
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "mnv2 = tf.keras.applications.DenseNet201(input_shape=IMG_SHAPE,\n",
        "                                         weights='imagenet',\n",
        "                                         include_top=False)\n",
        "\n",
        "mnv2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRLjD1fGdV5c"
      },
      "source": [
        "It is often a good idea to \"freeze\" the trained model. This prevents its weights from being updated when we train our new model.\n",
        "\n",
        "It is really only recommended to update the weights of the pretrained model when you are about to train on a large and similar dataset, as compared to the one that was originally trained on. This is not the case in our example. `ImageNet` has a thousand classes and over a million images. We have two classes and a few thousand images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JizhNdUdV5c"
      },
      "source": [
        "mnv2.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqGMpYGldV5c"
      },
      "source": [
        "##### Batching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzyT6zeOdV5c"
      },
      "source": [
        "We will want to train our model in batches. In our case we'll use a batch size of `32`. You might want to experiment with other sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_frm_GXSdV5c"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "\n",
        "train_batches = train_standardized.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "validation_batches = validation_standardized.batch(BATCH_SIZE)\n",
        "test_batches = test_standardized.filter(filter_out_small).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWCqJlpbdV5c"
      },
      "source": [
        "You can see that we now have a well-defined input shape for each training batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQdpoxLRdV5d"
      },
      "source": [
        "image_batch, label_batch = next(iter(train_batches.take(1)))\n",
        "\n",
        "image_batch.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1KeJ54ndV5d"
      },
      "source": [
        "If we apply our model to our first batch, you can see that we get a `(32, 7, 7, 1280)` block of features. These will be the input to our cat/dog model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blSatdpCdV5d"
      },
      "source": [
        "feature_batch = mnv2(image_batch)\n",
        "print(feature_batch.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYSNLATtdV5d"
      },
      "source": [
        "##### Extending the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2E8LOW0dV5d"
      },
      "source": [
        "Now we can perform the actual transfer learning. We'll build a new model that classifies images as containing dogs or cats. In order to do that, we can use a `Sequential` model with the pretrained model as the first layer.\n",
        "\n",
        "Note that the output layer of our pretrained model is:\n",
        "\n",
        "```text\n",
        "out_relu (ReLU)                 (None, 7, 7, 1280)   0           Conv_1_bn[0][0]  \n",
        "```\n",
        "\n",
        "Since the activation function is `relu6`, we know that the data that we'll be receiving is in the range of `[0, 6]`. We apply a pooling layer to reduce our inputs. In our output layer, we distill the inputs down to a single number that indicates if the image is of a cat or dog. We chose the sigmoid function, which will cause the output to be in a range of `[0, 1]`. This represents the confidence in an image being a dog, since dog is encoded as `1`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4_zsRGsdV5d"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  mnv2,\n",
        "  tf.keras.layers.GlobalAveragePooling2D(),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTAJpGs-dV5d"
      },
      "source": [
        "We now compile the model, training for accuracy with binary cross entropy used to calculate loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWqEdsgxdV5d"
      },
      "source": [
        "model.compile(\n",
        "  loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-30wv0EdV5d"
      },
      "source": [
        "Training will take a few minutes. Be sure to use GPU or it will take a really long time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRtveyP2dV5d"
      },
      "source": [
        "history = model.fit(\n",
        "  train_batches,#.maponehot(label, 101) # num is 101\n",
        "  epochs=10,\n",
        "  validation_data=validation_batches)\n",
        "  #validation_data=validation_batches.map(onehot(labels,num)))), # find the num of labels for this\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeywwBfpdV5e"
      },
      "source": [
        "We got a training accuracy of over `99%` and a validation accuracy close to `99%`! Let's graph the accuracy and loss per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFTgMRXJdV5e"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_BdAPL8dV5e"
      },
      "source": [
        "The graph makes it look like we might be overfitting, but if you look at the range on the y-axis, we actually aren't doing too badly. We should, however, perform a final test to see if we can generalize well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkRm8HszdV5e"
      },
      "source": [
        "model.evaluate(test_batches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLiEErqhdV5e"
      },
      "source": [
        "We got an accuracy of just over `99%`, which can give us some confidence that this model generalizes well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSR9UXIjdV5e"
      },
      "source": [
        "##### Making Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl_qJh0kdV5e"
      },
      "source": [
        "We can use the model to make predictions by using the `predict()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YGNmYW9dV5e"
      },
      "source": [
        "predictions = model.predict(test_batches)\n",
        "\n",
        "predictions.min(), predictions.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJbjtRXXdV5e"
      },
      "source": [
        "Remember the predictions can range from `0.0` to `1.0`. We can round them and cast them to integers to get class mappings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg78LexWdV5f"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "predictions = np.round(predictions.flatten(), 0).astype(np.int)\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBxSdh-mdV5f"
      },
      "source": [
        "And we can now print the predicted class alongside the original image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWbFrucZdV5f"
      },
      "source": [
        "print(get_class_name(predictions[0]))\n",
        "_ = plt.imshow(next(iter(raw_test.take(1)))[0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QulQ3pEhdV5g"
      },
      "source": [
        "You can also make predictions by calling the model directly and passing it a single batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEU_fsfgdV5g"
      },
      "source": [
        "predictions = model(next(iter(test_batches)))\n",
        "predictions = np.round(predictions, 0).astype(np.int).flatten()\n",
        "\n",
        "print(get_class_name(predictions[0]))\n",
        "_ = plt.imshow(next(iter(raw_test.take(1)))[0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asAB1I45g_2o"
      },
      "source": [
        "---"
      ]
    }
  ]
}