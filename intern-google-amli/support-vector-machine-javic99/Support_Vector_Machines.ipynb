{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Support Vector Machines",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "copyright"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJWsDPX6cIC0"
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTotGjoicova"
      },
      "source": [
        "# Support Vector Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTOkXjCecolX"
      },
      "source": [
        "Support Vector Machines (SVM) are powerful tools for performing both classification and regression tasks. In this colab we'll create a classification model using an SVM in scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxkLg1AkNtGb"
      },
      "source": [
        "## Load the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEKsqN3jwYfm"
      },
      "source": [
        "Let's begin by loading a dataset that we'll use for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXTshi78c4iv"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris_bunch = load_iris()\n",
        "\n",
        "iris_df = pd.DataFrame(iris_bunch.data, columns=iris_bunch.feature_names)\n",
        "iris_df['species'] = iris_bunch.target\n",
        "\n",
        "iris_df.describe() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BkQvJdEwuOl"
      },
      "source": [
        "You can see in the data description above that the range of values for each of the columns is quite a bit different. For instance, the mean sepal length is almost twice as big as the mean sepal width.\n",
        "\n",
        "SVM is sensitive to features with different scales. We'll run the data through the `StandardScaler` to get all of the feature data scaled.\n",
        "\n",
        "First let's create the scalar and fit it to our features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ecXA8GndgrP"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(iris_df[iris_bunch.feature_names])\n",
        "\n",
        "scaler.mean_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz8AZsCtxVRb"
      },
      "source": [
        "We can now transform the data by applying the `scaler`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYBrKIUbeauS"
      },
      "source": [
        "iris_df[iris_bunch.feature_names] = scaler.transform(\n",
        "    iris_df[iris_bunch.feature_names])\n",
        "\n",
        "iris_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKlDUcz0xr36"
      },
      "source": [
        "Since we scaled the data, the column names are now a bit deceiving. These are no longer unaltered centimeters, but normalized lengths. Let's rename the columns to get \"(cm)\" out of the names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uobpnTYfiTr7"
      },
      "source": [
        "iris_df = iris_df.rename(index=str, columns={\n",
        "  'sepal length (cm)': 'sepal_length',\n",
        "  'sepal width (cm)': 'sepal_width',\n",
        "  'petal length (cm)': 'petal_length',\n",
        "  'petal width (cm)': 'petal_width'})\n",
        "iris_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EauTJlfmx5FA"
      },
      "source": [
        "We could use all of the features to train our model, but in this case we are going to pick two features so that we can make some nice visualizations later on in the colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JukPMMb2ivPT"
      },
      "source": [
        "features = ['petal_length', 'petal_width']\n",
        "target = 'species'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n8DYftpogB2"
      },
      "source": [
        "iris_df[features]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfZKRDJwyLRm"
      },
      "source": [
        "Now we can create and train a classifier. There are multiple ways to create an SVM model in scikit-learn. We are going to use the [linear support vector classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weVHIEeue5xM"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "classifier = LinearSVC()\n",
        "classifier.fit(iris_df[features], iris_df[target])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YdEuhgWyhFO"
      },
      "source": [
        "We can now use our model to make predictions. We'll make predictions on the data we just trained on in order to get an F1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swJb81CdgIRe"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "predictions = classifier.predict(iris_df[features])\n",
        "\n",
        "f1_score(iris_df[target], predictions, average='micro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5eMo6VRyyfH"
      },
      "source": [
        "We can visualize the decision boundaries using the pyplot `contourf` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsuT2Fc7g0b0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Find the smallest value in the feature data. We are looking across both\n",
        "# features since we scaled them. Make the min value a little smaller than\n",
        "# reality in order to better see all of the points on the chart.\n",
        "min_val = min(iris_df[features].min()) - 0.25\n",
        "\n",
        "# Find the largest value in the feature data. Make the max value a little bigger\n",
        "# than reality in order to better see all of the points on the chart.\n",
        "max_val = max(iris_df[features].max()) + 0.25\n",
        "\n",
        "# Create a range of numbers from min to max with some small step. This will be\n",
        "# used to make multiple predictions that will create the decision boundary\n",
        "# outline.\n",
        "rng = np.arange(min_val, max_val, .02)\n",
        "\n",
        "# Create a grid of points.\n",
        "xx, yy = np.meshgrid(rng, rng)\n",
        "\n",
        "# Make predictions on every point in the grid.\n",
        "predictions = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Reshape the predictions for plotting.\n",
        "zz = predictions.reshape(xx.shape)\n",
        "\n",
        "# Plot the predictions on the grid.\n",
        "plt.contourf(xx, yy, zz)\n",
        "\n",
        "# Plot each class of iris with a different marker.\n",
        "#   Class 0 with circles\n",
        "#   Class 1 with triangles\n",
        "#   Class 2 with squares\n",
        "for species_and_marker in ((0, 'o'), (1, '^'), (2, 's')):\n",
        "  plt.scatter(\n",
        "    iris_df[iris_df[target] == species_and_marker[0]][features[0]],\n",
        "    iris_df[iris_df[target] == species_and_marker[0]][features[1]],\n",
        "    marker=species_and_marker[1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Tg74ogrczAA"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kvNGHbE1oCr"
      },
      "source": [
        "## Exercise 1: Polynomial SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1VfxQIclnA1"
      },
      "source": [
        "The scikit-learn module also has an [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) classifier that can use non-linear kernels. Create an `SVC` classifier with a 3-degree polynomial kernel, and train it on the iris data. Make predictions on the iris data that you trained on, and then print out the F1 score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFj-ndSgOSXG"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsYnJaHajGlc"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "classifier = SVC(kernel='poly')\n",
        "classifier.fit(iris_df[features], iris_df[target])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0opkDsMi-45"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "predictions = classifier.predict(iris_df[features])\n",
        "\n",
        "f1_score(iris_df[target], predictions, average='micro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQCTfdK2Xc7J"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHrAE0uP17Pb"
      },
      "source": [
        "## Exercise 2: Plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtwiXlXVlpA3"
      },
      "source": [
        "Create a plot that shows the decision boundaries of the polynomial SVC that you created in exercise 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-LvggLkOgn_"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199xq34kOoHz"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Find the smallest value in the feature data. We are looking across both\n",
        "# features since we scaled them. Make the min value a little smaller than\n",
        "# reality in order to better see all of the points on the chart.\n",
        "min_val = min(iris_df[features].min()) - 0.25\n",
        "\n",
        "# Find the largest value in the feature data. Make the max value a little bigger\n",
        "# than reality in order to better see all of the points on the chart.\n",
        "max_val = max(iris_df[features].max()) + 0.25\n",
        "\n",
        "# Create a range of numbers from min to max with some small step. This will be\n",
        "# used to make multiple predictions that will create the decision boundary\n",
        "# outline.\n",
        "rng = np.arange(min_val, max_val, .02)\n",
        "\n",
        "# Create a grid of points.\n",
        "xx, yy = np.meshgrid(rng, rng)\n",
        "\n",
        "# Make predictions on every point in the grid.\n",
        "predictions = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Reshape the predictions for plotting.\n",
        "zz = predictions.reshape(xx.shape)\n",
        "\n",
        "# Plot the predictions on the grid.\n",
        "plt.contourf(xx, yy, zz)\n",
        "\n",
        "# Plot each class of iris with a different marker.\n",
        "#   Class 0 with circles\n",
        "#   Class 1 with triangles\n",
        "#   Class 2 with squares\n",
        "for species_and_marker in ((0, 'o'), (1, '^'), (2, 's')):\n",
        "  plt.scatter(\n",
        "    iris_df[iris_df[target] == species_and_marker[0]][features[0]],\n",
        "    iris_df[iris_df[target] == species_and_marker[0]][features[1]],\n",
        "    marker=species_and_marker[1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM72MxryXjQW"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocYP9cb-28Hk"
      },
      "source": [
        "## Exercise 3: C Hyperparameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiH7HqPalrS4"
      },
      "source": [
        "We accepted the default 1.0 C hyperparameter in the classifier above. Try halving and doubling the C value. How does it affect the F1 score?\n",
        "\n",
        "Visualize the decision boundaries. Do they visibly change?\n",
        "\n",
        "*Yes, the boundaries change \"curvature\". Values > 1 create more curves along the boundaries and values < 1 have less curves and look more linear. Lower C values decrease the F1 score and higher C values increase the F1 score.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdckx6PUOzX0"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GRkvd4F3i4r"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "classifier = SVC(kernel='poly', C=2)\n",
        "classifier.fit(iris_df[features], iris_df[target])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8BG8CJrkwyz"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "predictions = classifier.predict(iris_df[features])\n",
        "\n",
        "f1_score(iris_df[target], predictions, average='micro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sgWZv3Sj-W6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Find the smallest value in the feature data. We are looking across both\n",
        "# features since we scaled them. Make the min value a little smaller than\n",
        "# reality in order to better see all of the points on the chart.\n",
        "min_val = min(iris_df[features].min()) - 0.25\n",
        "\n",
        "# Find the largest value in the feature data. Make the max value a little bigger\n",
        "# than reality in order to better see all of the points on the chart.\n",
        "max_val = max(iris_df[features].max()) + 0.25\n",
        "\n",
        "# Create a range of numbers from min to max with some small step. This will be\n",
        "# used to make multiple predictions that will create the decision boundary\n",
        "# outline.\n",
        "rng = np.arange(min_val, max_val, .02)\n",
        "\n",
        "# Create a grid of points.\n",
        "xx, yy = np.meshgrid(rng, rng)\n",
        "\n",
        "# Make predictions on every point in the grid.\n",
        "predictions = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "# Reshape the predictions for plotting.\n",
        "zz = predictions.reshape(xx.shape)\n",
        "\n",
        "# Plot the predictions on the grid.\n",
        "plt.contourf(xx, yy, zz)\n",
        "\n",
        "# Plot each class of iris with a different marker.\n",
        "#   Class 0 with circles\n",
        "#   Class 1 with triangles\n",
        "#   Class 2 with squares\n",
        "for species_and_marker in ((0, 'o'), (1, '^'), (2, 's')):\n",
        "  plt.scatter(\n",
        "    iris_df[iris_df[target] == species_and_marker[0]][features[0]],\n",
        "    iris_df[iris_df[target] == species_and_marker[0]][features[1]],\n",
        "    marker=species_and_marker[1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7k6MPqvX1gJ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJGWvH4k3krS"
      },
      "source": [
        "## Exercise 4: Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBGddbZvvRT7"
      },
      "source": [
        "Use the [LinearSVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html) to predict Boston housing prices in the [Boston housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html). Hold out some test data and print your final RMSE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTERkp24O63j"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRQxe3aFlPx-"
      },
      "source": [
        "#### Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB7vIgVylPyP"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "boston_bunch = load_boston()\n",
        "\n",
        "boston_df = pd.DataFrame(boston_bunch.data, columns=boston_bunch.feature_names)\n",
        "boston_df['PRICE'] = boston_bunch.target\n",
        "\n",
        "boston_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPUgNG4imjAj"
      },
      "source": [
        "#### Scale data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieLcEziflPyQ"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(boston_df[boston_bunch.feature_names])\n",
        "\n",
        "scaler.mean_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdUiBaBtlPyQ"
      },
      "source": [
        "boston_df[boston_bunch.feature_names] = scaler.transform(\n",
        "    boston_df[boston_bunch.feature_names])\n",
        "\n",
        "boston_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqXtDDUfmpW3"
      },
      "source": [
        "#### Pick best features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de8ss8r7lPyT"
      },
      "source": [
        "features = boston_df.columns[:-1].tolist()\n",
        "target = 'PRICE'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcRN0vjqRaI2"
      },
      "source": [
        "#### Train-Test-Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N81ONGpRX1Q"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    boston_df[features],\n",
        "    boston_df[target],\n",
        "    test_size=0.2\n",
        ")\n",
        "\n",
        "print(len(x_train), len(x_test), len(y_train), len(y_test))\n",
        "\n",
        "# we put these into our model for training\n",
        "  # x_train is the training features\n",
        "  # y_train is the training target\n",
        "\n",
        "# we put these into our model for predictions\n",
        "  # x_test is the testing features\n",
        "  # y_test is the testing target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTzdi1MTnHyi"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eZnn0lVlPyT"
      },
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "classifier = LinearSVR()\n",
        "classifier.fit(boston_df[features], boston_df[target])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwUPfOt40i_G"
      },
      "source": [
        "#### RMSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NMDe44hlPyU"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "predictions = classifier.predict(boston_df[features])\n",
        "\n",
        "mean_squared_error(boston_df[target], predictions, squared=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9JbV1mXZaYP"
      },
      "source": [
        "---"
      ]
    }
  ]
}