{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoders",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "copyright"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57VkR9qQDJFv"
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5W9rkuBmBu9"
      },
      "source": [
        "# Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIykBQbYXrXA"
      },
      "source": [
        "An **autoencoder** is a type of neural network used to learn an efficient representation, or encoding, for a set of data. The advantages of using these learned encodings are similar to those of word embeddings; they reduce the dimension of the feature space and can capture similarities between different inputs. Autoencoders are a useful *unsupervised* learning method, as they do not require any ground truth labels to train.\n",
        "\n",
        "This notebook is based on [this tutorial](https://github.com/mrdragonbear/Autoencoders/blob/master/Autoencoder-Tutorial.ipynb) and [this keras example](https://www.kaggle.com/vikramtiwari/autoencoders-using-tf-keras-mnist)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXdXx3uAD-yJ"
      },
      "source": [
        "## Data\n",
        "\n",
        "We will use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), which contains images of handwritten digits (0, 1, 2, etc.). This dataset has 60,000 training examples and 10,000 testing examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0vS0FyRB-5L"
      },
      "source": [
        "# Set random seeds for reproducible results.\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOjlzrQcmnx-"
      },
      "source": [
        "# Load dataset using keras data loader.\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ySzywmdkM2R"
      },
      "source": [
        "Each image in the dataset is 28 x 28 pixels. Let's flatten each to a 1-dimensional vector of length 784."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WdkcVjp5Ulz"
      },
      "source": [
        "image_size = x_train.shape[1]\n",
        "original_dim = image_size * image_size\n",
        "# Flatten each image into a 1-d vector.\n",
        "x_train = np.reshape(x_train, [-1, original_dim])\n",
        "x_test = np.reshape(x_test, [-1, original_dim])\n",
        "\n",
        "# Rescale pixel values to a 0-1 range.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "print('x_train:', x_train.shape)\n",
        "print('x_test:', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY9Ve8cjg258"
      },
      "source": [
        "## Autoencoder Structure\n",
        "\n",
        "\n",
        "<a title=\"Chervinskii [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)], via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Autoencoder_structure.png\"><img width=\"512\" alt=\"Autoencoder structure\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png\"></a>\n",
        "\n",
        "Source: [Wikipedia](https://en.wikipedia.org/wiki/Autoencoder)\n",
        "\n",
        "An autoencoder works by learning to output a copy of its input, after passing the input through one or more smaller hidden layer(s). This hidden layer describes an encoding or \"code\" used to represent the input (`x` in the above graph). An autoencoder has two main parts: an **encoder** that maps the input into the code, and a **decoder** that maps the code back to a reconstruction of the original input (`x'` in the above graph). This structure forces the hidden layer to learn a more efficient, useful representation of the input data (`z` in the above graph, also called a \"latent representation\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1OTBI25FAXm"
      },
      "source": [
        "## Basic Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWab3r6nrihv"
      },
      "source": [
        "Below is an example of a simple autoencoder that maps the 784-dimensional input image to a 36-dimensional latent representation, then attempts to reconstruct the 784-dimensional input image from that encoded representation. \n",
        "\n",
        "Instead of `keras.models.Sequential`, we'll use `keras.models.Model` to more clearly show the encoder and decoder parts of the autoencoder as individual models. This will also make it easier to extract the latent representations from the encoder. The `Sequential` API is usually easier to use while the `Model` API is more flexible. You can read more about their differences [here](https://keras.io/models/about-keras-models/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4xisJiEH_RF"
      },
      "source": [
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNx0ZRukv9kO"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeukWbOS6NY_"
      },
      "source": [
        "latent_dim = 36\n",
        "\n",
        "# input layer (needed for the Model API).\n",
        "input_layer = Input(shape=(original_dim,), name='encoder_input')\n",
        "\n",
        "# Notice that with all layers except for the first,\n",
        "# we need to specify which layer is used as input.\n",
        "latent_layer = Dense(latent_dim, activation='relu',\n",
        "                     name='latent_layer')(input_layer)\n",
        "\n",
        "encoder = Model(input_layer, latent_layer, name='encoder')\n",
        "encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baAnc2eov_Oq"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj4DN8vfwOwt"
      },
      "source": [
        "latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n",
        "output_layer = Dense(original_dim, name='decoder_output')(latent_inputs)\n",
        "\n",
        "decoder = Model(latent_inputs, output_layer, name='decoder')\n",
        "decoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6-tJnhsweO_"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVrVX8N8xsoa"
      },
      "source": [
        "The full autoencoder passes the inputs to the encoder, then the latent representations from the encoder to the decoder. We'll use the Adam optimizer and Mean Squared Error loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XR6JLDZ9Cym"
      },
      "source": [
        "autoencoder = Model(\n",
        "    input_layer,\n",
        "    decoder(encoder(input_layer)),\n",
        "    name=\"autoencoder\"\n",
        ")\n",
        "\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "autoencoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-lexigMqnnl"
      },
      "source": [
        "We will train for 50 epochs, using [`EarlyStopping`](https://keras.io/callbacks/#earlystopping) to stop training early if validation loss improves by less than 0.0001 for 10 consecutive epochs. Using a batch size of 2048, this should take 1-2 minutes to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkfBb8111Uey"
      },
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    # minimum change in loss that qualifies as \"improvement\"\n",
        "    # higher values of min_delta lead to earlier stopping\n",
        "    min_delta=0.0001,\n",
        "    # threshold for number of epochs with no improvement\n",
        "    patience=10,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpT4HphrsUiU"
      },
      "source": [
        "autoencoder.fit(\n",
        "    # input\n",
        "    x_train,\n",
        "    # output\n",
        "    x_train,\n",
        "    epochs=50,\n",
        "    batch_size=2048,\n",
        "    validation_data=(x_test, x_test),\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtlfBmWUuRPa"
      },
      "source": [
        "### Visualize Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACRu_c12_Pj-"
      },
      "source": [
        "decoded_imgs = autoencoder.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VbykAcH5nUL"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_imgs(nrows, axis_names, images, sizes, n=10):\n",
        "  '''\n",
        "  Plots images in a grid layout.\n",
        "\n",
        "  nrows: number of rows of images to display\n",
        "  axis_names: list of names for each row\n",
        "  images: list of arrays of images\n",
        "  sizes: list of image size to display for each row\n",
        "  n: number of images to display per row (default 10)\n",
        "\n",
        "  nrows = len(axis_names) = len(images)\n",
        "  '''\n",
        "  fig, axes = plt.subplots(figsize=(20,4), nrows=nrows, ncols=1, sharey=False)\n",
        "  for i in range(nrows):\n",
        "    axes[i].set_title(axis_names[i], fontsize=16)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "  for col in range(n):\n",
        "      for i in range(nrows):\n",
        "        ax = fig.add_subplot(nrows, n, col + 1 + i * n)\n",
        "        plt.imshow(images[i][col].reshape(sizes[i], sizes[i]))\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "  fig.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYR6ag7fRaze"
      },
      "source": [
        "visualize_imgs(\n",
        "    2,\n",
        "    ['Original Images', 'Reconstructions'],\n",
        "    [x_test, decoded_imgs],\n",
        "    [image_size, image_size]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiaWGUmwuhsg"
      },
      "source": [
        "This shows 10 original images with their corresponding reconstructed images directly below. Clearly, our autoencoder captured the basic digit structure of each image, though the reconstructed images are less sharp."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLYrydJwFIMk"
      },
      "source": [
        "## Application: Image Compression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvY97WJjEuWQ"
      },
      "source": [
        "Autoencoders have been used extensively in image compression and processing. An autoencoder can create higher resolution images from low-resolution images, and even colorize black and white images.\n",
        "\n",
        "To see how autoencoders can be used to compress images, we can use our already trained encoder as an image compressor. You can think of the decoder as a decompressor, reconstructing the original image from the compressed one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYGCIjE0wI3x"
      },
      "source": [
        "x_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUJfxl3qiukY"
      },
      "source": [
        "# Compress original images.\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "# Reconstruct original images.\n",
        "decoded_imgs = decoder.predict(encoded_imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr_tJH57H8kV"
      },
      "source": [
        "visualize_imgs(\n",
        "    3,\n",
        "    ['Original Images', '36-dimensional Latent Representation', 'Reconstructions'],\n",
        "    [x_test, encoded_imgs, decoded_imgs],\n",
        "    [image_size, 6, image_size]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIl9s4mcoHk7"
      },
      "source": [
        "Now we can visualize the latent representation of each image that the autoencoder learned. Since this reduces the 784-dimensional original image to a 36-dimensional image, it essentially performs an image compression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTohqWmn4mTt"
      },
      "source": [
        "## Application: Image Denoising"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpgeRwlVonCg"
      },
      "source": [
        "Autoencoders can also \"denoise\" images, such as poorly scanned pictures, and even partially damaged and destroyed paper documents ([Kaggle dataset](https://www.kaggle.com/c/denoising-dirty-documents)). To train a denoising autoencoder, we must first add noise to the images. \n",
        "\n",
        "*Note: \"Noise\" refers to something that interferes with the quality of original input, such as static in an image or a partially jumbled message.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDguvu3GowpE"
      },
      "source": [
        "### Add Noise\n",
        "\n",
        "[imgaug](https://github.com/aleju/imgaug) is a useful package to perform various image augmentations. Many of the `arithmetic` functions in the package simulate adding noise to an image. We'll use the [`SaltAndPepper`](https://imgaug.readthedocs.io/en/latest/source/api_augmenters_arithmetic.html#imgaug.augmenters.arithmetic.SaltAndPepper) technique.\n",
        "\n",
        "*Note: This will take slightly under a minute to run on the full training and testing sets.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ces5k7Po1D6"
      },
      "source": [
        "from imgaug import augmenters\n",
        "\n",
        "# Reshape images to 3-dimensional for augmenter. Since the images were\n",
        "# originally 2-dimensional, the third dimension is just 1.\n",
        "x_train = x_train.reshape(-1, image_size, image_size, 1)\n",
        "x_test = x_test.reshape(-1, image_size, image_size, 1)\n",
        "  \n",
        "# p is the probability of changing a pixel to noise.\n",
        "# higher values of p mean noisier images.\n",
        "noise = augmenters.SaltAndPepper(p=0.6)\n",
        "# We could chain multiple augmenters using Sequential.\n",
        "seq = augmenters.Sequential([noise])\n",
        "\n",
        "# Rescale pixel values to 0-255 (instead of 0-1) for augmenter,\n",
        "# add noise to images, then rescale pixel values back to 0-1.\n",
        "x_train_noise = seq.augment_images(x_train * 255) / 255\n",
        "x_test_noise = seq.augment_images(x_test * 255) / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s5hKlQ5pGkR"
      },
      "source": [
        "For comparison, here are what 5 images look like before we add noise:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2lGl4O8o-8Z"
      },
      "source": [
        "f, ax = plt.subplots(figsize=(20,2), nrows=1, ncols=5)\n",
        "for i in range(5, 10):\n",
        "    ax[i-5].imshow(x_train[i].reshape(image_size, image_size))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oCmCMFrqYkx"
      },
      "source": [
        "After we add noise, the images look like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27UBSEEOpK2z"
      },
      "source": [
        "f, ax = plt.subplots(figsize=(20,2), nrows=1, ncols=5)\n",
        "for i in range(5, 10):\n",
        "    ax[i-5].imshow(x_train_noise[i].reshape(image_size, image_size))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50GnzfOWrOSu"
      },
      "source": [
        "As you can see, the images are quite noisy and difficult to denoise even with the human eye. Luckily, autoencoders are much better at this task. We'll follow a similar architecture as before, but this time we'll train the model using the *noisy* images as input and the *original, un-noisy* images as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV7HNsCpF7tW"
      },
      "source": [
        "### Encoder\n",
        "\n",
        "We will need a more sophisticated encoder / decoder architecture to handle the more complex problem. The encoder will use 3 `Conv2D` layers, with decreasing output filter sizes and a `MaxPool` layer after each. This will perform the desired effect of compressing, or \"downsampling\", the image.\n",
        "\n",
        "Since we are using convolutional layers, we can work directly with the 3-dimensional images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WVA7hwsITcP"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, UpSampling2D\n",
        "\n",
        "filter_1 = 64\n",
        "filter_2 = 32\n",
        "filter_3 = 16\n",
        "kernel_size = (3, 3)\n",
        "pool_size = (2, 2)\n",
        "latent_dim = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPD93SbIqpKh"
      },
      "source": [
        "input_layer = Input(shape=(image_size, image_size, 1))\n",
        "# First convolutional layer\n",
        "encoder_conv1 = Conv2D(filter_1, kernel_size,\n",
        "                        activation='relu', padding='same')(input_layer)\n",
        "encoder_pool1 = MaxPool2D(pool_size, padding='same')(encoder_conv1)\n",
        "# Second convolutional layer\n",
        "encoder_conv2 = Conv2D(filter_2, kernel_size, activation='relu',\n",
        "                       padding='same')(encoder_pool1)\n",
        "encoder_pool2 = MaxPool2D(pool_size, padding='same')(encoder_conv2)\n",
        "# Third convolutional layer\n",
        "encoder_conv3 = Conv2D(filter_3, kernel_size,\n",
        "                       activation='relu', padding='same')(encoder_pool2)\n",
        "latent_layer = MaxPool2D(pool_size, padding='same')(encoder_conv3)\n",
        "\n",
        "encoder_denoise = Model(input_layer, latent_layer, name='encoder')\n",
        "encoder_denoise.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsZEY0R9IbDp"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "The decoder will work in reverse, using 3 `Conv2D` layers, with *increasing* output filter sizes and an [`UpSampling2D`](https://keras.io/layers/convolutional/#UpSampling2D) layer after each. This will perform the desired effect of reconstructing or denoising the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpAWZMOoIcOy"
      },
      "source": [
        "latent_inputs = Input(shape=(latent_dim, latent_dim, filter_3))\n",
        "\n",
        "# First convolutional layer\n",
        "decoder_conv1 = Conv2D(filter_3, kernel_size,\n",
        "                       activation='relu', padding='same')(latent_inputs)\n",
        "decoder_up1 = UpSampling2D(pool_size)(decoder_conv1)\n",
        "# Second convolutional layer\n",
        "decoder_conv2 = Conv2D(filter_2, kernel_size,\n",
        "                        activation='relu', padding='same')(decoder_up1)\n",
        "decoder_up2 = UpSampling2D(pool_size)(decoder_conv2)\n",
        "# Third convolutional layer\n",
        "decoder_conv3 = Conv2D(filter_1, kernel_size,\n",
        "                        activation='relu')(decoder_up2)\n",
        "decoder_up3 = UpSampling2D(pool_size)(decoder_conv3)\n",
        "\n",
        "# Output layer, which outputs images of size (28 x 28 x 1)\n",
        "output_layer = Conv2D(1, kernel_size, padding='same')(decoder_up3)\n",
        "\n",
        "decoder_denoise = Model(latent_inputs, output_layer, name='decoder')\n",
        "decoder_denoise.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3wyIKDfLL19"
      },
      "source": [
        "### Training\n",
        "\n",
        "We will again use early stopping and the same model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MvV5qQwrp-u"
      },
      "source": [
        "denoise_autoencoder = Model(\n",
        "    input_layer,\n",
        "    decoder_denoise(encoder_denoise(input_layer))\n",
        ")\n",
        "\n",
        "denoise_autoencoder.compile(optimizer='adam', loss='mse')\n",
        "denoise_autoencoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9vs0txfOqj_"
      },
      "source": [
        "We will only train for 10 epochs this time since the model is more complex and takes longer to train. This should take around a minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4zs0JGArrIr"
      },
      "source": [
        "denoise_autoencoder.fit(\n",
        "    # Input\n",
        "    x_train_noise,\n",
        "    # Output\n",
        "    x_train,\n",
        "    epochs=10,\n",
        "    batch_size=2048,\n",
        "    validation_data=(x_test_noise, x_test),\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dBfiFtoOw31"
      },
      "source": [
        "### Visualize Denoised Images\n",
        "\n",
        "Let's visualize the first 10 denoised images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueNg377ps8xe"
      },
      "source": [
        "denoised_imgs = denoise_autoencoder.predict(x_test_noise[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jcgAlpkr5t5"
      },
      "source": [
        "visualize_imgs(\n",
        "    3,\n",
        "    ['Noisy Images', 'Denoised Images', 'Original Images'],\n",
        "    [x_test_noise, denoised_imgs, x_test],\n",
        "    [image_size, image_size, image_size]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVBq82neP2zW"
      },
      "source": [
        "As we can see, the autoencoder is mostly successful in recovering the original image, though a few denoised images are still blurry or unclear. More training or a different model architecture may help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bskZFH5SgW2M"
      },
      "source": [
        "## Resources\n",
        "\n",
        "* [Introduction to Autoencoders](https://www.jeremyjordan.me/autoencoders)\n",
        "* [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
        "* [PCA vs. Autoencoders](https://towardsdatascience.com/pca-vs-autoencoders-1ba08362f450)\n",
        "* [Variational Autoencoders](https://www.jeremyjordan.me/variational-autoencoders/)\n",
        "* [Auto-Encoding Variational Bayes paper](https://arxiv.org/abs/1312.6114)\n",
        "* [Generating Images with VAEs](https://towardsdatascience.com/generating-images-with-autoencoders-77fd3a8dd368)\n",
        "* [Credit Card Fraud Detection using Autoencoders](https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd)\n",
        "* [Autoencoder Explained Video](https://www.youtube-nocookie.com/embed/H1AllrJ-_30?start=359)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swt2fxm-fG_B"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDWAMimv4YzH"
      },
      "source": [
        "## Exercise 1: Watermarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri2sWoE8Yehr"
      },
      "source": [
        "In this exercise we'll perform a task similar to the denoising in the example above. The [Mighty Mouse: Wolf! Wolf! dataset](https://www.kaggle.com/joshmcadams/mighty-mouse-wolf-wolf?select=mighty_mouse_160x120.mp4) contains a *Mighty Mouse* video that has been watermarked. In this exercise you'll create an autoencoder to remove the watermark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-epM6ab9Y00m"
      },
      "source": [
        "First download and unzip the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Noz3nvr9TpVl"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KST_EymnOzdY"
      },
      "source": [
        "! chmod 600 kaggle.json && (ls ~/.kaggle 2>/dev/null || mkdir ~/.kaggle) && cp kaggle.json ~/.kaggle/ && echo 'Done'\n",
        "! kaggle datasets download joshmcadams/mighty-mouse-wolf-wolf\n",
        "! unzip mighty-mouse-wolf-wolf.zip\n",
        "! ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3CrzyauY_Cf"
      },
      "source": [
        "We'll use the smaller videos (`80x60`) in this exercise in order to fit within Colab's memory limits and in order to get our model to run faster.\n",
        "\n",
        "`mighty_mouse_80x60_watermarked.mp4` contains the feature data. This is the watermarked video file.\n",
        "\n",
        "`mighty_mouse_80x60.mp4` contains the target data. This is the video file before watermarking.\n",
        "\n",
        "Your task is to build an autoencoder that can be used to restore the watermarked file back to a non-watermarked state.\n",
        "\n",
        "Use as many code and text cells as you need to. Explain your reasoning and work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OArJW9AXp9iA"
      },
      "source": [
        "### Read in videos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwwkoH0WMArG"
      },
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "watermarked_video = cv.VideoCapture('mighty_mouse_80x60_watermarked.mp4')\n",
        "original_video = cv.VideoCapture('mighty_mouse_80x60.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHhBlRVFgiBu"
      },
      "source": [
        "# watermarked\n",
        "height = int(watermarked_video.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
        "width = int(watermarked_video.get(cv.CAP_PROP_FRAME_WIDTH))\n",
        "fps = watermarked_video.get(cv.CAP_PROP_FPS)\n",
        "total_frames = int(watermarked_video.get(cv.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "print(f'height: {height}')\n",
        "print(f'width: {width}')\n",
        "print(f'frames per second: {fps}')\n",
        "print(f'total frames: {total_frames}')\n",
        "print(f'video length (seconds): {total_frames / fps}')\n",
        "\n",
        "# original\n",
        "height = int(original_video.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
        "width = int(original_video.get(cv.CAP_PROP_FRAME_WIDTH))\n",
        "fps = original_video.get(cv.CAP_PROP_FPS)\n",
        "total_frames = int(original_video.get(cv.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "print(f'height: {height}')\n",
        "print(f'width: {width}')\n",
        "print(f'frames per second: {fps}')\n",
        "print(f'total frames: {total_frames}')\n",
        "print(f'video length (seconds): {total_frames / fps}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqAanCcXvMVh"
      },
      "source": [
        "### Grab the frames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwzCP--ywWtM"
      },
      "source": [
        "# grabs the 30th frame from the video\n",
        "watermarked_frames = []\n",
        "for i in range(0, total_frames):\n",
        "  if i%300 == 0:\n",
        "    watermarked_video.set(cv.CAP_PROP_POS_FRAMES, i)\n",
        "    ret, frame = watermarked_video.read() # gets frame i from watermarked video\n",
        "    if not ret:\n",
        "      raise Exception(f\"Problem reading frame {i} from video\")\n",
        "    watermarked_frames.append(frame)\n",
        "    plt.imshow(frame)\n",
        "    plt.show()\n",
        "\n",
        "    #output_video.write(frame) # write the frame to the output video\n",
        "\n",
        "watermarked_video.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqpUcfry0hBp"
      },
      "source": [
        "# grabs the 30th frame from the video\n",
        "original_frames = []\n",
        "for i in range(0, total_frames):\n",
        "  if i%300 == 0:\n",
        "    original_video.set(cv.CAP_PROP_POS_FRAMES, i)\n",
        "    ret, frame = original_video.read() # gets frame i from watermarked video\n",
        "    if not ret:\n",
        "      raise Exception(f\"Problem reading frame {i} from video\")\n",
        "    original_frames.append(frame)\n",
        "    plt.imshow(frame)\n",
        "    plt.show()\n",
        "\n",
        "    #output_video.write(frame) # write the frame to the output video\n",
        "\n",
        "original_video.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaswMDB6zClN"
      },
      "source": [
        "watermarked_array = np.array(watermarked_frames)\n",
        "original_array = np.array(original_frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEG2VDW38ve6"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk2-60910d2-"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, UpSampling2D\n",
        "\n",
        "filter_1 = 64\n",
        "filter_2 = 32\n",
        "# filter_3 = 16\n",
        "kernel_size = (4, 4)\n",
        "pool_size = (2, 2)\n",
        "latent_dim = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTThNPQk0d2_"
      },
      "source": [
        "input_layer = Input(shape=(height, width, 3))\n",
        "# First convolutional layer\n",
        "encoder_conv1 = Conv2D(filter_1, kernel_size,\n",
        "                        activation='relu', padding='same')(input_layer)\n",
        "encoder_pool1 = MaxPool2D(pool_size, padding='same')(encoder_conv1)\n",
        "# Second convolutional layer\n",
        "encoder_conv2 = Conv2D(filter_2, kernel_size, activation='relu',\n",
        "                       padding='same')(encoder_pool1)\n",
        "latent_layer = MaxPool2D(pool_size, padding='same')(encoder_conv2)\n",
        "# Third convolutional layer\n",
        "# encoder_conv3 = Conv2D(filter_3, kernel_size,\n",
        "#                        activation='relu', padding='same')(encoder_pool2)\n",
        "# latent_layer = MaxPool2D(pool_size, padding='same')(encoder_conv3)\n",
        "\n",
        "encoder_denoise = Model(input_layer, latent_layer, name='encoder')\n",
        "encoder_denoise.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALigokMI82SR"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXzuaPmP01rU"
      },
      "source": [
        "latent_dim1=15\n",
        "latent_dim2=20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdCnBv6p0d3A"
      },
      "source": [
        "latent_inputs = Input(shape=(latent_dim1, latent_dim2, filter_2))\n",
        "\n",
        "# First convolutional layer\n",
        "decoder_conv1 = Conv2D(filter_2, kernel_size,\n",
        "                       activation='relu', padding='same')(latent_inputs)\n",
        "decoder_up1 = UpSampling2D(pool_size)(decoder_conv1)\n",
        "# Second convolutional layer\n",
        "decoder_conv2 = Conv2D(filter_1, kernel_size,\n",
        "                        activation='relu', padding='same')(decoder_up1)\n",
        "decoder_up2 = UpSampling2D(pool_size)(decoder_conv2)\n",
        "# Third convolutional layer\n",
        "# decoder_conv3 = Conv2D(filter_1, kernel_size,\n",
        "#                         activation='relu')(decoder_up2)\n",
        "# decoder_up3 = UpSampling2D(pool_size)(decoder_conv3)\n",
        "\n",
        "# Output layer, which outputs images of size (28 x 28 x 1)\n",
        "output_layer = Conv2D(3, kernel_size, padding='same')(decoder_up2)\n",
        "\n",
        "decoder_denoise = Model(latent_inputs, output_layer, name='decoder')\n",
        "decoder_denoise.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MUI_r9b0d3A"
      },
      "source": [
        "### Training\n",
        "\n",
        "We will again use early stopping and the same model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4Ik8YkJ0d3B"
      },
      "source": [
        "denoise_autoencoder = Model(\n",
        "    input_layer,\n",
        "    decoder_denoise(encoder_denoise(input_layer))\n",
        ")\n",
        "\n",
        "denoise_autoencoder.compile(optimizer='adam', loss='mse')\n",
        "denoise_autoencoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmmqGR52w662"
      },
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='loss',\n",
        "    # minimum change in loss that qualifies as \"improvement\"\n",
        "    # higher values of min_delta lead to earlier stopping\n",
        "    min_delta=0.0001,\n",
        "    # threshold for number of epochs with no improvement\n",
        "    patience=5,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZWVvn080d3B"
      },
      "source": [
        "We will only train for 10 epochs this time since the model is more complex and takes longer to train. This should take around a minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At5wVczC0d3B"
      },
      "source": [
        "denoise_autoencoder.fit(\n",
        "    # Input\n",
        "    watermarked_array,\n",
        "    # Output\n",
        "    original_array,\n",
        "    epochs=250,\n",
        "    batch_size=2048,\n",
        "    validation_data=(watermarked_array, original_array),\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImMqhXz7otyf"
      },
      "source": [
        "### Visualize Restored Video\n",
        "\n",
        "The model's loss is very bad which is why I think the frames are not displaying properly. I have the code to write the video its just the predicted frames don't look anything like the original video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2U6rc4ootyf"
      },
      "source": [
        "restored_imgs = denoise_autoencoder.predict(watermarked_array) # restored frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgSM-8lrysSU"
      },
      "source": [
        "plt.imshow(watermarked_array[5])\n",
        "plt.show()\n",
        "plt.imshow(original_array[5])\n",
        "plt.show()\n",
        "plt.imshow(restored_imgs[5])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFbG-CK6ohTj"
      },
      "source": [
        "# prepare output video\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fourcc = cv.VideoWriter_fourcc(*'mp4v')\n",
        "restored_video = cv.VideoWriter('restored.mp4', fourcc, fps, (width, height))\n",
        "\n",
        "# writes the frames to the video\n",
        "for i in range(0, len(restored_imgs)):\n",
        "  frame = restored_imgs[i]\n",
        "  print(frame)\n",
        "  print('hi')\n",
        "  plt.imshow(frame)\n",
        "  plt.show()\n",
        "\n",
        "  frame = (frame*255).astype(np.uint8)\n",
        "  print(frame)\n",
        "  plt.imshow(frame)\n",
        "  plt.show()\n",
        "  restored_video.write(frame) # write the frame to the output video\n",
        "  break\n",
        "\n",
        "restored_video.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFmeDHvsZbvF"
      },
      "source": [
        "---"
      ]
    }
  ]
}