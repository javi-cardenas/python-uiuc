{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Video Classification With Pre-Trained Models Project 4",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khlO4Bu21oZ4"
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3zN4BLMqYpo"
      },
      "source": [
        "# Group Members \n",
        "* **Patricia**\n",
        "* **Jalen**\n",
        "* **Javi**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlzIlBsScJJ_"
      },
      "source": [
        "# Video Classification with Pre-Trained Models Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTirVS4FWaPx"
      },
      "source": [
        "In this project we will import a pre-existing model that recognizes objects and use the model to identify those objects in a video. We'll edit the video to draw boxes around the identified object, and then we'll reassemble the video so the boxes are shown around objects in the video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTVUYxPwcHhp"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdIOgOHP1ces"
      },
      "source": [
        "## Exercise 1: Coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTEOK1ZmqN8"
      },
      "source": [
        "You will process a video frame by frame, identify objects in each frame, and draw a bounding box with a label around each car in the video.\n",
        " \n",
        "Use the [SSD MobileNet V1 Coco](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md) (*ssd_mobilenet_v1_coco*) model. The video you'll process can be found [on Pixabay](https://pixabay.com/videos/cars-motorway-speed-motion-traffic-1900/). The 640x360 version of the video is smallest and easiest to handle, though any size should work since you must scale down the images for processing.\n",
        " \n",
        "Your program should:\n",
        " \n",
        "* Read in a video file (use the one in this colab if you want)\n",
        "* Load the TensorFlow model linked above\n",
        "* Loop over each frame of the video\n",
        "* Scale the frame down to a size the model expects\n",
        "* Feed the frame to the model\n",
        "* Loop over detections made by the model\n",
        "* If the detection score is above some threshold, draw a bounding box onto the frame and put a label in or near the box\n",
        "* Write the frame back to a new video\n",
        " \n",
        "Some tips:\n",
        " \n",
        "* Processing an entire video is slow, so consider truncating the video or skipping over frames during development. Skipping frames will make the video choppy. But you'll be able to see a wider variety of images than you would with a truncated video with all of the original frames in the clip.\n",
        "* The model expects a 300x300 image. You'll likely have to scale your frames to fit the model. When you get a bounding box, that box is relative to the scaled image. You'll need to scale the bounding box out to the original image size.\n",
        "* Don't start by trying to process the video. Instead, capture one frame and work with it until you are happy with your object detection, bounding boxes, and labels. Once you get those done, use the same logic on the other frames of the video.\n",
        "* The [Coco labels file](https://github.com/nightrome/cocostuff/blob/master/labels.txt) can be used to identify classified objects.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XM35vYWSbim"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0Z3-prQMBph"
      },
      "source": [
        "#### Reading the Video\n",
        "\n",
        "OpenCV is an open source library for performing computer vision tasks. One of these tasks is reading and writing video frames. To read the `cars.mp4` video file, we use the [VideoCapture](https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture) class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwwkoH0WMArG"
      },
      "source": [
        "import cv2 as cv\n",
        "\n",
        "cars_video = cv.VideoCapture('cars.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzxNrZmigdz5"
      },
      "source": [
        "Once you have created a `VideoCapture` object, you can obtain information about the video that you are processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHhBlRVFgiBu"
      },
      "source": [
        "height = int(cars_video.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
        "width = int(cars_video.get(cv.CAP_PROP_FRAME_WIDTH))\n",
        "fps = cars_video.get(cv.CAP_PROP_FPS)\n",
        "total_frames = int(cars_video.get(cv.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "print(f'height: {height}')\n",
        "print(f'width: {width}')\n",
        "print(f'frames per second: {fps}')\n",
        "print(f'total frames: {total_frames}')\n",
        "print(f'video length (seconds): {total_frames / fps}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUQ5ZW6OhHgM"
      },
      "source": [
        "When you are done processing a video file, it is a good idea to release the VideoCapture to free up memory in your program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_Ubw7Wlgk52"
      },
      "source": [
        "cars_video.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBXgLAwHSlyN"
      },
      "source": [
        "#### Loading the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbEHbC5E-TaM"
      },
      "source": [
        "##### Download labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqjbmlAs-YFS"
      },
      "source": [
        "import urllib.request\n",
        "import os\n",
        "\n",
        "base_url = 'https://raw.githubusercontent.com/nightrome/cocostuff/master/'\n",
        "file_name = 'labels.txt'\n",
        "\n",
        "url = base_url + file_name\n",
        "\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aKbh9B_R6CE"
      },
      "source": [
        "###### Label dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSBx5RhvR5nq"
      },
      "source": [
        "labels = {}\n",
        "label_file = open('labels.txt')\n",
        "\n",
        "for line in label_file:\n",
        "  line = line[:-1]\n",
        "  key, value = line.split(': ')\n",
        "  key = int(key)\n",
        "  labels[key] = value\n",
        "\n",
        "print(labels)\n",
        "\n",
        "label_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD9qiB3C_0jZ"
      },
      "source": [
        "One of the most common places is the [official list](https://github.com/tensorflow/models) of pre-trained models curated by TensorFlow developers. This is often referred to as the *TensorFlow Model Garden*.\n",
        " \n",
        "In this course we will be utilizing models stored in the TensorFlow detection model zoo. The zoo has models built with [TensorFlow 1](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md) and [TensorFlow 2](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). We'll be focusing on the [Common Objects in Context (COCO)](http://cocodataset.org/) dataset. This dataset contains over 270,000 labeled images in 91 categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRW_C4qMCc8z"
      },
      "source": [
        "In order to use a pre-trained model, we first need to obtain the model file. For this Colab we'll visit the [TensorFlow detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md) and download the `ssd_mobilenet_v1_coco` model.\n",
        "\n",
        "The direct link to the model changes as the model is updated, so you'll need to browse the links in the zoo and find the model. Once you click the download link, you'll have a file on your system named similarly to (but not necessarily exactly the same as):\n",
        "\n",
        " > `ssd_mobilenet_v1_coco_2018_01_28.tar.gz`\n",
        "\n",
        "This is a compressed version of the model file. It is a gzipped (`.gz`) tape archive (`.tar`) file. If you want to explore the file on your local system, you might need to install a program such as [7-zip](https://www.7-zip.org/). On Mac and Linux systems you should be able to right-click on the file and extract the contents without any extra software. If you are comfortable with the command line, you can use the following command to extract the file contents.\n",
        "\n",
        "  > `tar -xzvf ssd_mobilenet_v1_coco_2018_01_28.tar.gz`\n",
        "\n",
        "And finally, if you just want to directly load the file to this Colab, update the file name in the code snippet below and run the code.\n",
        "\n",
        "Also notice that the documentation for the model we are downloading says the model was built using TensorFlow version 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDWpQ4Lf-KsB"
      },
      "source": [
        "##### Download model file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqpzWYtcFTmk"
      },
      "source": [
        "import urllib.request\n",
        "import os\n",
        "\n",
        "base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
        "file_name = 'ssd_mobilenet_v1_coco_2018_01_28.tar.gz'\n",
        "\n",
        "url = base_url + file_name\n",
        "\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuxLOHIJjBEU"
      },
      "source": [
        "##### Extract the Model Data (Unzip file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsfJLTVxSq7X"
      },
      "source": [
        "In order to load the model, it must be extracted from the compressed archive file (also called a \"tarball\" in this case). We will use Python's `tarfile` module to extract the contents of the file. The contents of the file will be saved in a directory named after the file. For example, the contents of `ssd_mobilenet_v1_coco_2018_01_28.tar.gz` will be saved in the `ssd_mobilenet_v1_coco_2018_01_28` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WGXDh3N8Nsr"
      },
      "source": [
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "dir_name = file_name[0:-len('.tar.gz')]\n",
        "\n",
        "if os.path.exists(dir_name):\n",
        "  shutil.rmtree(dir_name)\n",
        "\n",
        "tarfile.open(file_name, 'r:gz').extractall('./')\n",
        "\n",
        "os.listdir(dir_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1xn4IomjGf2"
      },
      "source": [
        "#### Loading the Frozen Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxOEoVzQUIIX"
      },
      "source": [
        "There are some interesting files contained in the archive, including checkpoints that can be used for resuming model training from a specific point. We care mostly about the `frozen_inference_graph.pb` file; this file contains a trained TensorFlow graph we can use for classification.\n",
        "\n",
        "We can load the frozen graph using TensorFlow's `GFile` method to open the file, then call `GraphDef.ParseFromString` to load the graph into memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdRv81cz2A_c"
      },
      "source": [
        "##### Import Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms8crLF9olg0"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-g_Wmbs-FKB"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "frozen_graph = os.path.join(dir_name, 'frozen_inference_graph.pb')\n",
        "\n",
        "with tf.io.gfile.GFile(frozen_graph, \"rb\") as f:\n",
        "  graph_def = tf.compat.v1.GraphDef()\n",
        "  loaded = graph_def.ParseFromString(f.read())\n",
        "\n",
        "import urllib.request"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ujy6B5mtkxjv"
      },
      "source": [
        "Remember that TensorFlow allows you to request execution to any node in the graph. We want to know all of the *detection* outputs that we discovered in the graph. These were:\n",
        "\n",
        "  * num_detections\n",
        "  * detection_scores\n",
        "  * detection_boxes\n",
        "  * detection_classes\n",
        "\n",
        "We will build a list of *outputs* that we want TensorFlow to generate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avXulXBVgk_P"
      },
      "source": [
        "##### Run the Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxmaAWYqgk_P"
      },
      "source": [
        "Now that we have a graph loaded, we need to test it out. Let's download an [image of a car](https://pixabay.com/illustrations/car-sports-car-racing-car-speed-49278/) and upload that image to Colab. Rename the file `car.jpg` or change the name of the `image_filename` variable below to match the name of the file you uploaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6wu1oOOgk_Q"
      },
      "source": [
        "outputs = (\n",
        "  'num_detections:0',\n",
        "  'detection_classes:0',\n",
        "  'detection_scores:0',\n",
        "  'detection_boxes:0',\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktn1aINmgk_Q"
      },
      "source": [
        "We can now execute the graph requesting our outputs and providing inputs.\n",
        "\n",
        "In order to do this, we must first wrap the graph. This is necessary due to compatibility issues between TensorFlow version 1 and 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK1vNUIIgk_Q"
      },
      "source": [
        "def wrap_graph(graph_def, inputs, outputs, print_graph=False):\n",
        "  wrapped = tf.compat.v1.wrap_function(\n",
        "    lambda: tf.compat.v1.import_graph_def(graph_def, name=\"\"), [])\n",
        "\n",
        "  return wrapped.prune(\n",
        "    tf.nest.map_structure(wrapped.graph.as_graph_element, inputs),\n",
        "    tf.nest.map_structure(wrapped.graph.as_graph_element, outputs))\n",
        "    \n",
        "model = wrap_graph(graph_def=graph_def,\n",
        "                   inputs=[\"image_tensor:0\"],\n",
        "                   outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEXxIokFgk_R"
      },
      "source": [
        "And then to make predictions, we convert our image into a tensor and pass it to the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf9mPaEBgk_R"
      },
      "source": [
        "# tensor = tf.convert_to_tensor(input_frame, dtype=tf.uint8)\n",
        "\n",
        "# detections = model(tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI6XkrYtgk_S"
      },
      "source": [
        "##### Main Code - Output Video and Detections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQIY5yfqgk_S"
      },
      "source": [
        "Loop over the detections of the model\n",
        "###### How do I access element of tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x5hu-_egk_S"
      },
      "source": [
        "# detections[0][0] # number of detections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgF3qyMEv2T1"
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# cars_video = cv.VideoCapture('cars.mp4')\n",
        "# cars_video.set(cv.CAP_PROP_POS_FRAMES, 850)\n",
        "# ret, frame = cars_video.read()\n",
        "# if not ret:\n",
        "#   raise Exception(f'Problem reading frame from video')\n",
        "\n",
        "# cars_video.release()\n",
        "\n",
        "# frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
        "# plt.imshow(frame)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej641gVExFhJ"
      },
      "source": [
        "# wrap Tensorflow 1 and Tensorflow 2\n",
        "def wrap_graph(graph_def, inputs, outputs, print_graph=False):\n",
        "  wrapped = tf.compat.v1.wrap_function(\n",
        "    lambda: tf.compat.v1.import_graph_def(graph_def, name=\"\"), [])\n",
        "\n",
        "  return wrapped.prune(\n",
        "    tf.nest.map_structure(wrapped.graph.as_graph_element, inputs),\n",
        "    tf.nest.map_structure(wrapped.graph.as_graph_element, outputs))\n",
        "    \n",
        "model = wrap_graph(graph_def=graph_def,\n",
        "                   inputs=[\"image_tensor:0\"],\n",
        "                   outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nkg5x9btv46E"
      },
      "source": [
        "# prepare output video\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "input_video = cv.VideoCapture('cars.mp4')\n",
        "\n",
        "height = int(input_video.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
        "width = int(input_video.get(cv.CAP_PROP_FRAME_WIDTH))\n",
        "fps = input_video.get(cv.CAP_PROP_FPS)\n",
        "\n",
        "fourcc = cv.VideoWriter_fourcc(*'mp4v')\n",
        "output_video = cv.VideoWriter('cars-project4.mp4', fourcc, fps, (width, height))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjnUdjzPwARL"
      },
      "source": [
        "# grabs the 8th frame from the video\n",
        "for i in range(0, total_frames):\n",
        "  if i%8 == 0:\n",
        "    input_video = cv.VideoCapture('cars.mp4')\n",
        "    input_video.set(cv.CAP_PROP_POS_FRAMES, i)\n",
        "    ret, frame = input_video.read() # gets frame i from video\n",
        "    if not ret:\n",
        "      raise Exception(f\"Problem reading frame {i} from video\")\n",
        "\n",
        "    input_frame = [frame] # resize frame for model\n",
        "\n",
        "    # make detections\n",
        "    tensor = tf.convert_to_tensor(input_frame, dtype=tf.uint8)\n",
        "    detections = model(tensor)\n",
        "\n",
        "    for i in range(100): # num_detections:0\n",
        "      confidence_score = detections[2][0][i] # detection_scores:0\n",
        "  \n",
        "      # only draw boundaries and labels for scores over 50%\n",
        "      if confidence_score > 0.5:\n",
        "        print(confidence_score)\n",
        "          \n",
        "        # draw boxes\n",
        "        box = detections[3][0][i] # detection_boxes:0\n",
        "        box_top = box[0]\n",
        "        box_left = box[1]\n",
        "        box_bottom = box[2]\n",
        "        box_right = box[3]\n",
        "\n",
        "        left = int(box_left * width)\n",
        "        right = int(box_right * width)\n",
        "        top = int(box_top * height)\n",
        "        bottom = int(box_bottom * height)\n",
        "\n",
        "        # black text and boundaries\n",
        "        r = 0\n",
        "        g = 0\n",
        "        b = 0\n",
        "\n",
        "        cv.rectangle(frame, (left, top), (right, bottom), (r, g, b), thickness=2)\n",
        "\n",
        "        # add text\n",
        "        label_id = int(detections[1][0][i]) # detection_classes:0\n",
        "        label_name = labels[label_id]\n",
        "\n",
        "        scale = 1.0\n",
        "        thickness = 2\n",
        "\n",
        "        # center_h = int((box_left + box_right) / 2)\n",
        "        # center_v = int((box_bottom + box_top) / 2)\n",
        "\n",
        "        cv.putText(frame, label_name, (right, bottom), cv.FONT_HERSHEY_SIMPLEX, scale,\n",
        "              [r, g, b], thickness)\n",
        "\n",
        "        \n",
        "        plt.imshow(frame)\n",
        "        plt.show()\n",
        "\n",
        "    output_video.write(frame) # write the frame to the output video\n",
        "  \n",
        "input_video.release()\n",
        "\n",
        "output_video.release()\n",
        "\n",
        "os.listdir('./')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvlTLftsrrhJ"
      },
      "source": [
        "We inspected our model and found that it accepts a list of variable-sized images and that it returns:\n",
        "- the number of matches\n",
        "- the class\n",
        "- the confidence\n",
        "- the bounding boxes for each object found in an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEGDiC-IhcrM"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HniKdSXg0YHR"
      },
      "source": [
        "## Exercise 2: Ethical Implications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4FvC1Aa0ZT5"
      },
      "source": [
        "Even the most basic models have the potential to affect segments of the population in different ways. It is important to consider how your model might positively and negatively affect different types of users.\n",
        "\n",
        "In this section of the project, you will reflect on the positive and negative implications of your model. Frame the context of your model creation using this narrative:\n",
        "\n",
        "> The city of Seattle is attempting to reduce traffic congestion in its downtown area. As part of this project, they plan to allow each local driver one free trip to downtown Seattle per week. After that, the driver will have to pay a $50 toll for each extra day per week driven. As an early proof of concept for this project, your team is tasked with using machine learning to correctly identify automobiles on the road. The next phase of the project will involve detecting license plate numbers and then cross-referencing that data with RFID chips that should be mounted in all local drivers' cars."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkyzwVQr0brd"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy4I2vG60ebd"
      },
      "source": [
        "**Positive Impact**\n",
        "\n",
        "Your model is trying to solve a problem. Think about who will benefit from that problem being solved and write a brief narrative about how the model will help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k59MK1Ah0fWy"
      },
      "source": [
        "> *Residents of Seattle will benefit with reduced traffic congestion and less smog since there will be less drivers on the road because of the $50 toll. This will hopefully incentivize carpooling or other forms of travel to downtown Seattle. Moslty the city of Seattle will benefit with more income coming from these tolls.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzqkrLnk0hMU"
      },
      "source": [
        "**Negative Impact**\n",
        "\n",
        "Models rarely benefit everyone equally. Think about who might be negatively impacted by the predictions your model is making. This person(s) might not be directly using the model, but they might be impacted indirectly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hefa1JdP0kj3"
      },
      "source": [
        "> *Regular commuters that have to travel to downtown Seattle will be negatively affected because of the $50 toll. This will most likely hurt people of lower-income since they won't be able to repeatedly pay for the toll.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uax2HAzd0mHX"
      },
      "source": [
        "**Bias**\n",
        "\n",
        "Models can be biased for many reasons. The bias can come from the data used to build the model (e.g., sampling, data collection methods, available sources) and/or from the interpretation of the predictions generated by the model.\n",
        "\n",
        "Think of at least two ways bias might have been introduced to your model and explain both below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bJGm-qs0oQV"
      },
      "source": [
        "> *One source of bias in the model could be... sample bias. Our sample is only a video of cars. What about the different types of vehicles that travel. Our model is expected to be considerably less accurate when it comes to motorcycles, SUV's, trucks, and 18-wheelers. Could our model detect an emergency vehicle and forego its payment*\n",
        "\n",
        "The interpretation of the predictions generated from the model, which has the the intention of reducing traffic congestion in the Seattle downtown area, could introduce bias in the sense that it could then generate more congestion in different areas. In an effort to avoid the $50 toll after using their free trip, this will create congestion in different areas. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybb1zAkC0p2e"
      },
      "source": [
        "**Changing the Dataset to Mitigate Bias**\n",
        "\n",
        "Having bias in your dataset is one of the primary ways in which bias is introduced to a machine learning model. Look back at the input data you fed to your model. Think about how you might change something about the data to reduce bias in your model.\n",
        "\n",
        "What change or changes could you make to reduce the bias in your dataset? Consider the data you have, how and where it was collected, and what other sources of data might be used to reduce bias.\n",
        "\n",
        "Write a summary of changes that could be made to your input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFsnF4_h08DD"
      },
      "source": [
        "> *Since the data has potential bias A we can adjust... our input data. Maybe create a model with more diverse vehicles. Possibly detecting their type and putting them in different categories to charge different tolls based on weight or vehicle emission*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChEJbhXA02pW"
      },
      "source": [
        "**Changing the Model to Mitigate Bias**\n",
        "\n",
        "Is there any way to reduce bias by changing the model itself? This could include modifying algorithmic choices, tweaking hyperparameters, etc.\n",
        "\n",
        "Write a brief summary of changes you could make to help reduce bias in your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEAhgO_U0p8Y"
      },
      "source": [
        "> *Since the model has potential bias A, we can adjust... increase the threshold for confidence from 50% to maybe 65%.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rShB5BQv0wix"
      },
      "source": [
        "**Mitigating Bias Downstream**\n",
        "\n",
        "Models make predictions. Downstream processes make decisions. What processes and/or rules should be in place for people and systems interpreting and acting on the results of your model to reduce bias? Describe these rules and/or processes below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C__BwBP-00HN"
      },
      "source": [
        "> Since the predictions have potential bias towards reading the license plate of cars, certain rules should be in place to make sure that no major laws/rules are being made on these results because it is not representative of all the vehicles riding on these roads. Taking action based on the results of the model could be somewhat detrimental to the communities to they are in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L_4RNXphYtI"
      },
      "source": [
        "---"
      ]
    }
  ]
}